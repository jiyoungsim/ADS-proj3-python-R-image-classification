{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Model--Boosted Decision Stump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries\n",
    "Set os.environ[\"R_USER\"] to user name in windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "import import_ipynb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"R_USER\"] = \"Jiyoung Sim\" # user name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2 import robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects import numpy2ri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Provide directories for training/testing images. Images and fiducial points will be in different subfolders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/train_set/' # This will be modified for different data sets.\n",
    "train_image_dir = train_dir + 'images/'\n",
    "train_pt_dir = train_dir + 'points/'\n",
    "train_label_path = train_dir + 'label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/test_set/' # This will be modified for different data sets.\n",
    "test_image_dir = train_dir + 'images/'\n",
    "test_pt_dir = train_dir + 'points/'\n",
    "test_label_path = train_dir + 'label.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: set up controls for evaluation experiments.\n",
    "Set baseline_tt_split = True if you want to do train_test_split for data in the same directory. Set it to False if you have test data in a different directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_feature_train = True # process features for training set\n",
    "baseline_train = True # train model\n",
    "baseline_cv= True # hyperparameter tuning by GridSearchCV when training\n",
    "baseline_feature_test = False # process features for test set\n",
    "baseline_test = False # run evaluation on an independent test set\n",
    "baseline_tt_split = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: import data and train-test split if wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(train_label_path)\n",
    "if(baseline_tt_split):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx_py, test_idx_py = train_test_split(range(len(info)), test_size=0.2, random_state = 0)\n",
    "    train_idx_r = [i+1 for i in train_idx_py]\n",
    "    test_idx_r = [i+1 for i in test_idx_py]\n",
    "    info_test = info\n",
    "else:\n",
    "    info_test = pd.read_csv(test_label_path)\n",
    "    train_idx_py = list(range(len(info)))\n",
    "    train_idx_r = [i+1 for i in train_idx_py]\n",
    "    test_idx_py = list(range(len(info_test)))\n",
    "    test_idx_r = [i+1 for i in test_idx_py]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct features and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature.R\n",
    "feature = robjects.r(\n",
    "    '''\n",
    "    source('../lib/feature.R')\n",
    "    '''\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read fiducial points\n",
    "def readMat(index):\n",
    "    import scipy.io\n",
    "    numpy2ri.activate()\n",
    "    try:\n",
    "        mat = np.round(scipy.io.loadmat(train_pt_dir + '{:04n}.mat'.format(index))['faceCoordinatesUnwarped'])\n",
    "    except KeyError:\n",
    "        mat = np.round(scipy.io.loadmat(train_pt_dir + '{:04n}.mat'.format(index))['faceCoordinates2'])\n",
    "    nr,nc = mat.shape\n",
    "    mat_r = robjects.r.matrix(mat, nrow=nr, ncol=nc)\n",
    "    robjects.r.assign(\"mat\", mat_r)\n",
    "    return mat_r\n",
    "\n",
    "#load fiducial points\n",
    "start = time.time()\n",
    "n_files = len(os.listdir(train_pt_dir))\n",
    "fiducial_pt_list = [readMat(index) for index in range(1, n_files+1)]\n",
    "end = time.time()\n",
    "tm_fid_pt_train = end-start\n",
    "\n",
    "if(baseline_tt_split):\n",
    "    fiducial_pt_test = fiducial_pt_list\n",
    "    tm_fid_pt_test = tm_fid_pt_train\n",
    "else:\n",
    "    start = time.time()\n",
    "    n_files = len(os.listdir(test_pt_dir))\n",
    "    fiducial_pt_test = [readMat(index) for index in range(1, n_files+1)]\n",
    "    end = time.time()\n",
    "    tm_fid_pt_test = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas dataframe to R dataframe\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "info_rdf = pandas2ri.py2ri(info)\n",
    "info_test_rdf = pandas2ri.py2ri(info_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiyoung Sim\\Anaconda3\\lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:191: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  res = PandasDataFrame.from_items(items)\n"
     ]
    }
   ],
   "source": [
    "# extract features from fiducial points\n",
    "as_factor = robjects.r('''as.factor''')\n",
    "if(baseline_feature_train):\n",
    "    start = time.time()\n",
    "    dat_train_r = feature(fiducial_pt_list, train_idx_r, info_rdf)\n",
    "    end = time.time()\n",
    "    dat_train_py = pandas2ri.ri2py_dataframe(dat_train_r)\n",
    "    dat_train_r[-1] = as_factor(dat_train_r[-1])\n",
    "    tm_feature_train_baseline = end - start + tm_fid_pt_train\n",
    "#     dat_train_py.to_csv('dat_train_py.csv', index=False)\n",
    "\n",
    "if(baseline_feature_test):\n",
    "    start = time.time()\n",
    "    dat_test_r = feature(fiducial_pt_test, test_idx_r, info_test_rdf)\n",
    "    end = time.time()\n",
    "    dat_test_py = pandas2ri.ri2py_dataframe(dat_test_r)\n",
    "    dat_test_r[-1] = as_factor(dat_test_r[-1])\n",
    "    tm_feature_test_baseline = end - start + tm_fid_pt_test\n",
    "#     dat_test_py.to_csv('dat_test_py.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a classification model with training features and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from train_baseline.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiyoung Sim\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed: 34.9min remaining: 30.5min\n"
     ]
    }
   ],
   "source": [
    "# train baseline model\n",
    "baseline_dir = 'baseline_train_alldata.sav'#'baseline_train_main.sav' \n",
    "if (baseline_train==True):\n",
    "    import train_baseline\n",
    "    tm_train_baseline, baseline = train_baseline.gbm_fn(dat_train_py.iloc[:,:-1], dat_train_py.iloc[:,-1], baseline_cv)\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(baseline, baseline_dir) # save the model to disk\n",
    "\n",
    "# test\n",
    "if (baseline_test==True):\n",
    "    import test_baseline\n",
    "    start= time.time()\n",
    "    baseline_acc = test_baseline.test_clf(dat_test_py.iloc[:,:-1], dat_test_py.iloc[:,-1], baseline_dir) \n",
    "    end = time.time()\n",
    "    tm_test_baseline = end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Summarize Running Time and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training feature extraction took: 2.1819183826446533\n",
      "testing feature extraction took: 2.1819183826446533\n",
      "model training took: 972.6331098079681\n",
      "model training took: 0.9105648994445801\n",
      "model accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "print('training feature extraction took: {}'.format(tm_feature_train_baseline))\n",
    "print('testing feature extraction took: {}'.format(tm_feature_train_baseline))\n",
    "print('model training took: {}'.format(tm_train_baseline))\n",
    "print('model testing took: {}'.format(tm_test_baseline))\n",
    "print('model accuracy: {}'.format(baseline_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improved Model--Voting Classifier (Combines Light GBM (dart), Logistic Regression, Linear SVM, and Random Forest)\n",
    "### Step 1: set up controls for evaluation experiments.\n",
    "Set baseline_tt_split = True if you want to do train_test_split for data in the same directory. Set it to False if you have test data in a different directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_feature_train = True # process features for training set\n",
    "voting_train = False # train model\n",
    "voting_cv= False # hyperparameter tuning by GridSearchCV when training\n",
    "voting_feature_test = True # process features for test set\n",
    "voting_test = True # run evaluation on an independent test set\n",
    "voting_tt_split = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: import data and train-test split if wanted--identical to Step 2 in previous part\n",
    "### Step 3: construct features and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import myfeature2.R\n",
    "myfeature2 = robjects.r(\n",
    "    '''\n",
    "    source('../lib/myfeature2.R')\n",
    "    '''\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from fiducial points\n",
    "start = time.time()\n",
    "myfeature_train_r = myfeature2(info_rdf, fiducial_pt_list)\n",
    "myfeature_train_py = pandas2ri.ri2py_dataframe(myfeature_train_r)\n",
    "myfeature_train_r[-1] = as_factor(myfeature_train_r[-1])\n",
    "end = time.time()\n",
    "tm_feature_train_voting = end - start + tm_fid_pt_train\n",
    "\n",
    "start = time.time()\n",
    "myfeature_test_r = myfeature2(info_test_rdf, fiducial_pt_test)\n",
    "myfeature_test_py = pandas2ri.ri2py_dataframe(myfeature_test_r)\n",
    "myfeature_test_r[-1] = as_factor(myfeature_test_r[-1])\n",
    "end = time.time()\n",
    "tm_feature_test_voting = end - start + tm_fid_pt_test\n",
    "\n",
    "if(voting_tt_split):   \n",
    "    train_df = myfeature_train_py.iloc[train_idx_py].reset_index(drop=True)\n",
    "    test_df = myfeature_train_py.iloc[test_idx_py].reset_index(drop=True)\n",
    "else:\n",
    "    train_df = my_feature_train_py\n",
    "    test_df = my_feature_test_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a classification model with training features and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from train_voting.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiyoung Sim\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from test_voting.ipynb\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'test_voting' has no attribute 'test_test_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d51eb3c30a98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mtest_voting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mvoting_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_voting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_test_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoting_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtm_test_voting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'test_voting' has no attribute 'test_test_fn'"
     ]
    }
   ],
   "source": [
    "# train improved model\n",
    "voting_dir = 'voting_train_alldata.sav'#'voting_train_main.sav' \n",
    "if (voting_train==True):\n",
    "    import train_voting\n",
    "    tm_train_voting, voting = train_voting.train_fn(train_df.iloc[:,:-1], train_df.iloc[:,-1], voting_cv)\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(voting, voting_dir) # save the model to disk\n",
    "\n",
    "# test the model\n",
    "if (voting_test==True):\n",
    "    import test_voting\n",
    "    start= time.time()\n",
    "    voting_acc = test_voting.test_fn(test_df.iloc[:,:-1], test_df.iloc[:,-1], voting_dir) \n",
    "    end = time.time()\n",
    "    tm_test_voting = end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Summarize Running Time and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training feature extraction took: 2.2154464721679688\n",
      "testing feature extraction took: 2.2154464721679688\n",
      "model training took: 168.33163595199585\n",
      "model training took: 2.0300350189208984\n",
      "model accuracy: 0.474\n"
     ]
    }
   ],
   "source": [
    "print('training feature extraction took: {}'.format(tm_feature_train_voting))\n",
    "print('testing feature extraction took: {}'.format(tm_feature_train_voting))\n",
    "print('model training took: {}'.format(tm_train_voting))\n",
    "print('model training took: {}'.format(tm_test_voting))\n",
    "print('model accuracy: {}'.format(voting_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
